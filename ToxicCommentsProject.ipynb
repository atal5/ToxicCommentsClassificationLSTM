{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comments Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective - To classify each comment text to either of the following 6 comment types\n",
    "#### One comment  can belong to multiple of these types\n",
    "\n",
    "- Toxic\n",
    "- Severe_toxic\n",
    "- Obscene\n",
    "- Threat\n",
    "- Insult\n",
    "- Indetity_Hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Approach - Use a LSTM based Neural Network to predict probabilities for a comment belongin to each of the 6 classification types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath = \"./data/\"\n",
    "\n",
    "train = pd.read_csv(dataPath + 'train.csv',index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes in the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "types = list(train)[1:]\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ac9276a5f8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[types].mean().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of clean comments in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean: 143346 ( 89.83 % )\n"
     ]
    }
   ],
   "source": [
    "nb_clean = (train[types].sum(axis=1)==0).sum()\n",
    "print('clean:',nb_clean,'(',round(100*nb_clean/len(train),2),'% )')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some comments in the test set were labelled after the kaggle competition ended. Therefore moving those labelled comments from the test set to the train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "id                                                                           \n",
      "00001cee341fdb12     -1            -1       -1      -1      -1             -1\n",
      "0000247867823ef7     -1            -1       -1      -1      -1             -1\n",
      "00013b17ad220c46     -1            -1       -1      -1      -1             -1\n",
      "00017563c3f7919a     -1            -1       -1      -1      -1             -1\n",
      "00017695ad8997eb     -1            -1       -1      -1      -1             -1\n",
      "0.41770912224804785 % of test is labelled\n",
      "(223549, 7) (89186, 1)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(dataPath+'test.csv',index_col=0)\n",
    "test_labels = pd.read_csv(dataPath+'test_labels.csv',index_col=0)\n",
    "print(test_labels.head())\n",
    "labelled_test = test.join(test_labels)\n",
    "disclosed = labelled_test.toxic>-1\n",
    "print(disclosed .mean(),'% of test is labelled')\n",
    "train = train.append(labelled_test[disclosed])\n",
    "test = labelled_test[~disclosed][['comment_text']]\n",
    "print(train.shape,test.shape)\n",
    "train.to_csv(dataPath+'tc_train.csv')\n",
    "test.to_csv(dataPath+'tc_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New train and test shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223549, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89186, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding layer using fastext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding index from file in .txt format. First line contains \n",
    "# dictionary size and embedding dim. Fields are space separated\n",
    "def get_embeddings(file_name):\n",
    "    embeddings_index = {}\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            # remove white spaces at the end and split\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) > 2:\n",
    "                embeddings_index[values[0]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = './Embeddings1/crawl-300d-2M.vec/'\n",
    "embeddings_index = get_embeddings(embeddings_path+'crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'and', 'to', 'of', 'a', 'in', 'is', 'for', 'that', 'I', 'it', 'on', 'with', ')', ':', '\"', '(', 'The', 'you', 'was', 'are', 'or', 'this']\n",
      "\n",
      "First key:  , \n",
      "\n",
      "Vector length:  300 \n",
      "\n",
      "Vector for first key:  [-2.820e-02 -5.570e-02 -4.510e-02 -4.340e-02  7.120e-02 -8.550e-02\n",
      " -1.085e-01 -5.610e-02 -4.523e-01 -2.020e-02  9.750e-02  1.047e-01\n",
      "  1.962e-01 -6.930e-02  2.130e-02 -2.350e-02  1.336e-01 -4.200e-02\n",
      " -5.640e-02 -7.980e-02  4.240e-02 -4.090e-02 -5.360e-02 -2.520e-02\n",
      "  1.350e-02  6.400e-03  1.235e-01  4.610e-02  1.200e-02 -3.720e-02\n",
      "  6.500e-02  4.100e-03 -1.074e-01 -2.630e-02  1.133e-01 -2.900e-03\n",
      "  6.710e-02  1.065e-01  2.340e-02 -1.600e-02  7.000e-03  4.355e-01\n",
      " -7.520e-02 -4.328e-01  4.570e-02  6.040e-02 -7.400e-02 -5.500e-03\n",
      " -8.900e-03 -2.926e-01 -5.450e-02 -1.519e-01  9.900e-02 -1.930e-02\n",
      " -5.000e-03  5.110e-02  4.040e-02  1.023e-01 -1.280e-02  4.880e-02\n",
      " -1.567e-01 -7.590e-02 -1.900e-02  1.442e-01  4.700e-03 -1.860e-02\n",
      "  1.400e-02 -3.850e-02 -8.530e-02  1.572e-01  1.770e-01  8.400e-03\n",
      " -2.500e-02 -1.145e-01 -6.630e-02 -1.244e-01 -3.977e-01 -1.240e-02\n",
      " -4.586e-01 -2.200e-02  5.746e-01  2.180e-02 -7.540e-02  9.900e-03\n",
      "  3.970e-02 -1.540e-02  4.240e-02 -1.500e-02 -1.600e-03  3.050e-02\n",
      "  1.010e-02  2.266e-01  1.394e-01  1.890e-02  6.900e-03  3.940e-02\n",
      "  3.550e-02 -1.110e-02 -6.870e-02 -7.800e-03  2.240e-02  8.170e-02\n",
      " -1.949e-01  1.000e-04  4.047e-01 -2.370e-02 -6.560e-02 -6.840e-02\n",
      "  2.330e-02  4.380e-02  1.203e-01 -2.760e-02  4.160e-02  1.140e-02\n",
      " -4.529e-01  1.538e-01  1.323e-01 -1.860e-02 -9.140e-02 -3.120e-02\n",
      "  1.051e-01  2.120e-02  7.980e-02 -1.040e-02 -2.060e-02 -2.500e-03\n",
      "  4.300e-03 -3.780e-02  2.689e-01  7.470e-02 -4.180e-02 -4.800e-03\n",
      " -3.870e-02  4.320e-02  1.704e-01  6.140e-02  9.050e-02 -4.360e-02\n",
      " -1.410e-02 -3.150e-02  2.760e-02  1.510e-02 -1.030e-02 -2.660e-02\n",
      " -5.120e-02 -4.080e-02 -6.510e-02  6.620e-02 -9.360e-02  1.371e-01\n",
      "  4.580e-02 -1.366e-01 -7.500e-03 -1.040e-02 -7.320e-02  1.205e-01\n",
      "  1.035e-01  1.060e-02 -3.170e-02 -3.160e-02  6.639e-01 -2.200e-03\n",
      " -1.343e-01  1.440e-02 -3.380e-02  3.400e-03 -4.290e-02 -8.210e-02\n",
      "  3.700e-03  1.029e-01 -2.040e-02 -2.690e-02  5.200e-03 -1.034e-01\n",
      "  1.068e-01  1.210e-02  9.800e-02 -4.580e-02  1.990e-02 -1.320e-02\n",
      "  1.936e-01 -2.130e-02  2.090e-02 -2.500e-03  4.160e-02 -3.370e-02\n",
      "  5.160e-02 -1.014e-01  2.030e-02  1.980e-02 -3.050e-02 -3.130e-02\n",
      "  5.430e-02 -1.060e-02  1.441e-01 -1.780e-02 -6.270e-02  4.750e-02\n",
      "  3.520e-02 -2.540e-02 -9.490e-02  4.010e-02  3.170e-02  5.500e-03\n",
      " -5.360e-02  1.910e-02 -5.110e-02 -4.090e-02 -3.000e-03  1.582e-01\n",
      "  1.080e-02  5.237e-01  4.360e-02  3.060e-02 -3.920e-02  1.770e-02\n",
      "  6.900e-03  6.050e-02  1.206e-01 -2.160e-02 -6.330e-02 -2.965e-01\n",
      "  5.210e-02 -1.500e-02 -2.207e-01 -6.420e-02 -9.060e-02 -1.210e-02\n",
      "  5.690e-02  9.440e-02 -6.520e-02 -1.080e-02 -4.770e-02  2.300e-03\n",
      "  7.700e-03 -1.547e-01  4.630e-02  6.980e-02 -3.760e-02 -2.910e-02\n",
      "  3.300e-03 -1.020e-02 -7.430e-02  8.500e-03  8.050e-02 -2.910e-02\n",
      " -6.740e-02 -5.860e-02 -6.530e-02  2.830e-02 -2.550e-02  8.690e-02\n",
      " -8.680e-02  9.000e-03  3.245e-01 -5.730e-02 -2.890e-02  4.700e-02\n",
      " -1.170e-02  1.740e-02  1.320e-02 -2.260e-02 -6.640e-02  1.880e-02\n",
      "  2.630e-02  1.110e-02 -4.900e-03 -6.560e-02  2.950e-02  4.350e-02\n",
      "  2.900e-02  1.163e-01  4.480e-02 -1.139e-01 -5.530e-02 -5.280e-02\n",
      "  1.745e-01 -1.460e-02 -1.308e-01 -6.070e-02 -1.340e-02  7.810e-02\n",
      "  3.780e-02  2.280e-02 -7.280e-02 -5.900e-03  1.580e-02 -1.410e-02\n",
      " -2.000e-04  1.930e-02 -1.480e-02 -4.630e-02  4.440e-02  3.034e-01\n",
      "  1.020e-01 -8.710e-02  3.170e-02 -3.700e-02 -7.250e-02 -4.200e-03]\n"
     ]
    }
   ],
   "source": [
    "print(list(embeddings_index.keys())[:25])\n",
    "ke,va = list(embeddings_index.items())[0]\n",
    "print('\\nFirst key: ',ke,'\\n\\nVector length: ',len(va),'\\n\\nVector for first key: ',va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove all newline, carriage return, digits and punctuations except apostrophe from the text. Also remove an y consecutivey white spaces and replace it with single white space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "trans_table = str.maketrans({key: ' ' for key in string.digits + '\\r\\n' +\n",
    "                             string.punctuation.replace(\"\\'\",'')})\n",
    "def preprocess(text):\n",
    "    return ' '.join(text.lower().translate(trans_table).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts text to token counts.<br>\n",
    "- Fit function learns all the unique words from teh text array and assigns a unique index to each word.<br>\n",
    "- Then transform function can be used to count words in each new string and update the count of the index of that word<br>\n",
    "- Q - what if the word is not there in new text?<br>\n",
    "- A - then that word is not considered in the count<br>\n",
    "Therefore the count vectorizer should be fit to the whole data set including train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 5\n",
      "45259 top words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "UNKNOWN_PROXY = 'unknown'\n",
    "MIN_WORD_OCCURRENCE = 5\n",
    "\n",
    "train['comment_text'] = train.comment_text.apply(preprocess)\n",
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", \n",
    "                             min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(train.comment_text)\n",
    "\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(UNKNOWN_PROXY)\n",
    "print(len(top_words),'top words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 out of \"top_words\": \n",
      " ['overpopulation', 'disscusion', 'deputies', 'endorse', 'mistress', 'filmmaking', 'ac', 'serpent', 'ruth', 'illness']\n",
      "\n",
      "Is \"unknown\" in top_words? \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "print('First 10 out of \"top_words\": \\n',list(top_words)[:10])\n",
    "print('\\nIs \"unknown\" in top_words? \\n','unknown' in top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark the words which are not in vocabulary and also not in the embeddings downloaded from FASTTEXT and replace them with text `'unknown'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknown(text, vocabulary, proxy):\n",
    "    return ' '. \\\n",
    "join([w if w in vocabulary else proxy for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text']  = train.comment_text.apply(filter_unknown,\n",
    "                args=(set(embeddings_index.keys() & top_words), \\\n",
    "                      UNKNOWN_PROXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x000002AD73391A58>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(train.comment_text)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('unknown', 1),\n",
       " ('the', 2),\n",
       " ('to', 3),\n",
       " ('of', 4),\n",
       " ('and', 5),\n",
       " ('a', 6),\n",
       " ('you', 7),\n",
       " ('i', 8),\n",
       " ('is', 9),\n",
       " ('that', 10),\n",
       " ('in', 11),\n",
       " ('it', 12),\n",
       " ('for', 13),\n",
       " ('this', 14),\n",
       " ('not', 15),\n",
       " ('on', 16)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "list(word_index.items())[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 sequences in `seq`:  [[668, 77, 2, 133, 130, 176, 31, 658, 4322, 10972, 1109, 85, 347, 1, 52, 1, 12090, 1, 51, 6172, 16, 60, 2522, 147, 8, 2744, 34, 116, 1179, 15205, 2412, 1, 5, 48, 61, 245, 2, 357, 32, 2, 42, 28, 142, 1, 3422, 89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 53, 2714, 14, 463, 3572, 1, 4414, 2647, 22, 1, 93, 1, 1, 1, 42, 1, 1, 1, 1, 1, 1, 1, 1, 953, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 195, 1]]\n",
      "\n",
      "Shape of `data`:  (223549, 50)\n",
      "\n",
      "First prepared text in `data`: [  668    77     2   133   130   176    31   658  4322 10972  1109    85\n",
      "   347     1    52     1 12090     1    51  6172    16    60  2522   147\n",
      "     8  2744    34   116  1179 15205  2412     1     5    48    61   245\n",
      "     2   357    32     2    42    28   142     1  3422    89     1     1\n",
      "     1     1]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "seq = tokenizer.texts_to_sequences(train.comment_text)\n",
    "data = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                     truncating='post')\n",
    "with open(dataPath + 'toxic_comments.pkl','wb') as f: pickle.dump(data, f, -1)\n",
    "\n",
    "print('\\nFirst 2 sequences in `seq`: ',seq[:2])\n",
    "print('\\nShape of `data`: ',data.shape)\n",
    "print('\\nFirst prepared text in `data`:',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dim = len(next(iter(embeddings_index.values())))\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "def get_embedding_matrix(word_index,embeddings_index):\n",
    "    nb_words = len(word_index) + 1 # +1 since min(word_index.values())=1\n",
    "    embedding_matrix = np.zeros((nb_words,embeddings_dim))\n",
    "    unknown = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is None: unknown += 1\n",
    "        else: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 13:39:29.355118  6324 deprecation_wrapper.py:119] From c:\\programdata\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 unknown words\n"
     ]
    }
   ],
   "source": [
    "# Create embedding_layer and save it.\n",
    "def make_save_emb_layer(word_index,embeddings_index,layer_file_name):\n",
    "    embedding_matrix,unknown = get_embedding_matrix(word_index,embeddings_index)\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],trainable=False)\n",
    "    with open(layer_file_name,'wb') as f: \n",
    "        pickle.dump(embedding_layer, f, -1)\n",
    "    return unknown\n",
    "\n",
    "EMBEDDING_LAYER_FILE = dataPath + 'TOXIC_COMMENTS_EMB_LAYER.pkl'\n",
    "print(make_save_emb_layer(word_index,embeddings_index,EMBEDDING_LAYER_FILE),\n",
    "      'unknown words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Embedding,Input,Dropout,Conv1D\n",
    "from keras.layers import SpatialDropout1D, Flatten,LSTM,GlobalAveragePooling1D,GlobalMaxPooling1D,concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    x = LSTM(20, return_sequences=True)(x)\n",
    "    x = Conv1D(20, kernel_size=2, padding=\"valid\")(x)\n",
    "    y = GlobalMaxPooling1D()(x)\n",
    "    t = GlobalAveragePooling1D()(x)\n",
    "    x = concatenate([y,t],name='merge')\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>unknown unknown he matches this background col...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>hey man unknown unknown really not trying to e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>unknown unknown more i can't make any real sug...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>you unknown sir unknown are my hero unknown an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  explanation why the edits made under my userna...      0   \n",
       "000103f0d9cfb60f  unknown unknown he matches this background col...      0   \n",
       "000113f07ec002fd  hey man unknown unknown really not trying to e...      0   \n",
       "0001b41b1c6bb37e  unknown unknown more i can't make any real sug...      0   \n",
       "0001d958c54c6e35  you unknown sir unknown are my hero unknown an...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      " id\n",
      "0000997932d777bf    000000\n",
      "000103f0d9cfb60f    000000\n",
      "000113f07ec002fd    000000\n",
      "0001b41b1c6bb37e    000000\n",
      "0001d958c54c6e35    000000\n",
      "dtype: object\n",
      "\n",
      "Counts of labels: \n",
      " 000000    201081\n",
      "100000      7376\n",
      "101010      5732\n",
      "101000      2612\n",
      "100010      1754\n",
      "111010      1165\n",
      "101011       979\n",
      "111011       381\n",
      "001000       366\n",
      "000010       365\n",
      "100011       215\n",
      "100001       203\n",
      "101110       196\n",
      "001010       196\n",
      "111000       186\n",
      "100100       163\n",
      "111110        88\n",
      "101111        81\n",
      "000001        68\n",
      "101001        55\n",
      "111111        45\n",
      "110000        41\n",
      "000011        32\n",
      "000100        27\n",
      "100110        25\n",
      "001011        19\n",
      "101100        17\n",
      "110010        14\n",
      "100101        11\n",
      "110100        11\n",
      "111100         8\n",
      "110011         7\n",
      "111001         7\n",
      "rare           5\n",
      "110101         5\n",
      "000110         4\n",
      "001001         3\n",
      "100111         3\n",
      "110001         3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# convert each vector of labels to the string by concatenating\n",
    "labels = train[types].astype(str).apply(lambda x: ''.join(x),axis=1)\n",
    "print('Labels: \\n',labels.head())\n",
    "# aggregate rare combinations if any\n",
    "count = labels.value_counts()\n",
    "rare = count.index[count<=2]\n",
    "labels[np.isin(labels.values,rare)] = 'rare'\n",
    "print('\\nCounts of labels: \\n',labels.value_counts())\n",
    "train_index, val_index = train_test_split(train.index, test_size=0.2, \n",
    "                                      stratify = labels, random_state=0)\n",
    "# save train and validation indices for further calculations\n",
    "fname = dataPath + 'train_val_split.pkl'\n",
    "with open(fname, 'wb') as f: pickle.dump([train_index, val_index], f, -1),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_LAYER_FILE, 'rb') as f: embedding_layer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      12055800    input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_10 (SpatialDr (None, 50, 300)      0           embedding_1[9][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 50, 20)       25680       spatial_dropout1d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 49, 20)       820         lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 20)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 20)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "merge (Concatenate)             (None, 40)           0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 40)           160         merge[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 40)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 6)            246         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,082,706\n",
      "Trainable params: 26,826\n",
      "Non-trainable params: 12,055,880\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model.h5'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "model_checkpoint = ModelCheckpoint(best_model_path,\n",
    "                                   save_best_only=True, save_weights_only=True)\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='toxic_comments.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPath + 'toxic_comments.pkl', 'rb') as f: data = pickle.load(f) \n",
    "with open(dataPath + 'train_val_split.pkl', 'rb') as f: data_idx = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 668,   77,    2, ...,    1,    1,    1],\n",
       "       [   1,    1,   53, ...,    0,    0,    0],\n",
       "       [ 437,  415,    1, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    1, 2026, ...,    0,    0,    0],\n",
       "       [6555, 6886,    9, ...,    0,    0,    0],\n",
       "       [   1,    1,    1, ...,    1,    1,   12]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223549, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178839, 44710)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_idx[0]),len(data_idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train.index.isin(data_idx[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  668,    77,     2,   133,   130,   176,    31,   658,  4322,\n",
       "       10972,  1109,    85,   347,     1,    52,     1, 12090,     1,\n",
       "          51,  6172,    16,    60,  2522,   147,     8,  2744,    34,\n",
       "         116,  1179, 15205,  2412,     1,     5,    48,    61,   245,\n",
       "           2,   357,    32,     2,    42,    28,   142,     1,  3422,\n",
       "          89,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0000997932d777bf', '000103f0d9cfb60f', '000113f07ec002fd',\n",
       "       '0001b41b1c6bb37e', '0001d958c54c6e35', '00025465d4725e87',\n",
       "       '0002bcb3da6cb337', '00031b1e95af7921', '00037261f536c51d',\n",
       "       '00040093b2687caa',\n",
       "       ...\n",
       "       'fff69311f306df44', 'fff7159b3ee95618', 'fff718ffe5f05559',\n",
       "       'fff83b80284d8440', 'fff8f521a7dbcd47', 'fff8f64043129fa2',\n",
       "       'fff9d70fe0722906', 'fffa8a11c4378854', 'fffac2a094c8e0e2',\n",
       "       'fffb5451268fb5ba'],\n",
       "      dtype='object', name='id', length=223549)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[train.index.isin(data_idx[0].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data[train.index.isin(data_idx[1].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.iloc[train.index.isin(data_idx[0].values),1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val=train.iloc[train.index.isin(data_idx[1].values),1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178839, 50), (178839, 6), (44710, 50), (44710, 6))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178839 samples, validate on 44710 samples\n",
      "Epoch 1/50\n",
      " - 30s - loss: 0.3442 - val_loss: 0.0885\n",
      "Epoch 2/50\n",
      " - 28s - loss: 0.0797 - val_loss: 0.0601\n",
      "Epoch 3/50\n",
      " - 28s - loss: 0.0679 - val_loss: 0.0561\n",
      "Epoch 4/50\n",
      " - 28s - loss: 0.0639 - val_loss: 0.0550\n",
      "Epoch 5/50\n",
      " - 28s - loss: 0.0613 - val_loss: 0.0537\n",
      "Epoch 6/50\n",
      " - 28s - loss: 0.0600 - val_loss: 0.0545\n",
      "Epoch 7/50\n",
      " - 27s - loss: 0.0592 - val_loss: 0.0527\n",
      "Epoch 8/50\n",
      " - 28s - loss: 0.0582 - val_loss: 0.0524\n",
      "Epoch 9/50\n",
      " - 28s - loss: 0.0573 - val_loss: 0.0523\n",
      "Epoch 10/50\n",
      " - 28s - loss: 0.0568 - val_loss: 0.0520\n",
      "Epoch 11/50\n",
      " - 28s - loss: 0.0563 - val_loss: 0.0522\n",
      "Epoch 12/50\n",
      " - 27s - loss: 0.0559 - val_loss: 0.0515\n",
      "Epoch 13/50\n",
      " - 28s - loss: 0.0557 - val_loss: 0.0517\n",
      "Epoch 14/50\n",
      " - 27s - loss: 0.0550 - val_loss: 0.0516\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train,validation_data=(X_val, y_val),\n",
    "                 epochs=50, batch_size=BATCH_SIZE, shuffle=True, verbose=2,\n",
    "                 callbacks=[model_checkpoint, early_stopping])\n",
    "model.load_weights(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC 0.9782969360974284\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "print('validation AUC',roc_auc_score(y_val, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8616190e-02, 2.5317073e-04, 3.4365356e-03, 4.2429566e-04,\n",
       "        6.1927736e-03, 9.4911456e-04],\n",
       "       [1.3948232e-02, 1.0100007e-04, 1.9225776e-03, 1.5437603e-04,\n",
       "        2.8411150e-03, 5.2976608e-04],\n",
       "       [2.3182720e-02, 2.3519993e-04, 3.3714473e-03, 4.6390295e-04,\n",
       "        6.4124465e-03, 6.0430169e-04],\n",
       "       ...,\n",
       "       [6.6319108e-04, 1.8805265e-05, 3.7053227e-04, 1.8507242e-05,\n",
       "        1.0150671e-04, 7.0631504e-06],\n",
       "       [3.9142370e-04, 7.6293945e-06, 1.1059642e-04, 4.1723251e-06,\n",
       "        6.9111586e-05, 1.2457371e-05],\n",
       "       [7.0174038e-03, 5.0902367e-05, 1.1603534e-03, 1.3923645e-04,\n",
       "        1.2416543e-03, 1.5310785e-04]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 5\n",
      "25113 top words\n",
      "First 10 out of \"top_words\": \n",
      " ['overpopulation', 'endorse', 'mistress', 'ac', 'ruth', 'illness', 'point', 'categorize', 'corpus', 'implants']\n",
      "\n",
      "Is \"unknown\" in top_words? \n",
      " True\n",
      "\n",
      "First 2 sequences in `seq`:  [[2223, 470, 5992, 678, 9, 59, 18143, 83, 853, 355, 17, 3324, 73, 22, 7, 5, 5919, 7, 1486, 1, 1, 1, 8, 58, 470, 5067, 1698, 1, 603, 5843, 5, 94, 7, 3, 4368, 31, 398, 7, 748, 33797, 38, 1, 5992, 678, 9, 36, 3904, 11, 1546, 639, 415, 1, 488, 19416, 10, 312, 16, 154, 1, 5, 22398, 9, 241, 23218, 50, 15133, 53, 25, 6, 2054, 157, 1, 1, 2602, 603, 2540, 94, 218, 144, 481, 84, 1, 1], [1, 1, 1, 32, 1108, 1, 1, 1, 1, 1, 1, 2, 330, 9, 649, 18, 12, 9, 1, 2606, 1]]\n",
      "\n",
      "Shape of `data`:  (89186, 50)\n",
      "\n",
      "First prepared text in `data`: [ 2223   470  5992   678     9    59 18143    83   853   355    17  3324\n",
      "    73    22     7     5  5919     7  1486     1     1     1     8    58\n",
      "   470  5067  1698     1   603  5843     5    94     7     3  4368    31\n",
      "   398     7   748 33797    38     1  5992   678     9    36  3904    11\n",
      "  1546   639]\n"
     ]
    }
   ],
   "source": [
    "UNKNOWN_PROXY = 'unknown'\n",
    "MIN_WORD_OCCURRENCE = 5\n",
    "\n",
    "test['comment_text'] = test.comment_text.apply(preprocess)\n",
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", \n",
    "                             min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(test.comment_text)\n",
    "\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(UNKNOWN_PROXY)\n",
    "print(len(top_words),'top words')\n",
    "\n",
    "print('First 10 out of \"top_words\": \\n',list(top_words)[:10])\n",
    "print('\\nIs \"unknown\" in top_words? \\n','unknown' in top_words)\n",
    "\n",
    "test['comment_text']  = test.comment_text.apply(filter_unknown,\n",
    "                args=(set(embeddings_index.keys() & top_words), \\\n",
    "                      UNKNOWN_PROXY))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "seq = tokenizer.texts_to_sequences(test.comment_text)\n",
    "data = pad_sequences(seq,maxlen=MAX_SEQUENCE_LENGTH,padding='post',\n",
    "                     truncating='post')\n",
    "#with open(dataPath + 'toxic_comments.pkl','wb') as f: pickle.dump(data, f, -1)\n",
    "\n",
    "print('\\nFirst 2 sequences in `seq`: ',seq[:2])\n",
    "print('\\nShape of `data`: ',data.shape)\n",
    "print('\\nFirst prepared text in `data`:',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89186, 50)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_c_pred = model.predict(data, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9516308e-01, 5.6663412e-01, 9.8607445e-01, 1.5909609e-01,\n",
       "        9.4867778e-01, 2.8173754e-01],\n",
       "       [1.1588931e-03, 6.8008900e-05, 3.9207935e-04, 6.3210726e-05,\n",
       "        3.4731627e-04, 8.1002712e-05],\n",
       "       [2.5899112e-03, 1.9606948e-04, 1.1016130e-03, 8.1419945e-05,\n",
       "        7.6174736e-04, 2.4750829e-04],\n",
       "       ...,\n",
       "       [2.6371181e-03, 1.5234947e-04, 8.4653497e-04, 1.2481213e-04,\n",
       "        6.4206123e-04, 2.5492907e-04],\n",
       "       [2.7304292e-03, 1.5819073e-04, 8.5338950e-04, 1.8885732e-04,\n",
       "        8.8396668e-04, 5.0842762e-04],\n",
       "       [8.4190166e-01, 1.3477027e-02, 5.7922614e-01, 2.1764338e-03,\n",
       "        4.4918689e-01, 5.9066713e-03]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_c_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(test_c_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['id']=test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995163</td>\n",
       "      <td>0.566634</td>\n",
       "      <td>0.986074</td>\n",
       "      <td>0.159096</td>\n",
       "      <td>0.948678</td>\n",
       "      <td>0.281738</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \\\n",
       "0  0.995163      0.566634  0.986074  0.159096  0.948678       0.281738   \n",
       "1  0.001159      0.000068  0.000392  0.000063  0.000347       0.000081   \n",
       "2  0.002590      0.000196  0.001102  0.000081  0.000762       0.000248   \n",
       "3  0.000657      0.000025  0.000501  0.000046  0.000443       0.000111   \n",
       "4  0.004571      0.000144  0.000825  0.000080  0.000724       0.000100   \n",
       "\n",
       "                 id  \n",
       "0  00001cee341fdb12  \n",
       "1  0000247867823ef7  \n",
       "2  00013b17ad220c46  \n",
       "3  00017563c3f7919a  \n",
       "4  00017695ad8997eb  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.995163</td>\n",
       "      <td>0.566634</td>\n",
       "      <td>0.986074</td>\n",
       "      <td>0.159096</td>\n",
       "      <td>0.948678</td>\n",
       "      <td>0.281738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.995163      0.566634  0.986074  0.159096  0.948678   \n",
       "1  0000247867823ef7  0.001159      0.000068  0.000392  0.000063  0.000347   \n",
       "2  00013b17ad220c46  0.002590      0.000196  0.001102  0.000081  0.000762   \n",
       "3  00017563c3f7919a  0.000657      0.000025  0.000501  0.000046  0.000443   \n",
       "4  00017695ad8997eb  0.004571      0.000144  0.000825  0.000080  0.000724   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.281738  \n",
       "1       0.000081  \n",
       "2       0.000248  \n",
       "3       0.000111  \n",
       "4       0.000100  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = result.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "result = result[cols]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('ToxicSubmission6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correct!\n",
    "Well done!\n",
    "\n",
    "Your AUC = 0.965213. Must be not less than threshold = 0.980000. Score = 94."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
